% !TeX root = ../my-thesis.tex
\chapter{Untersuchte Konzepte und Implementierungen}

\section{Theoretische Aspekte in die Parallelität}
Das Ziel hinter der Parallelisierung von Aufgaben ist die Beschleunigung der Laufzeit bei der Abarbeitung von Programmabläufen und die Minimierung der Wartezeiten des Prozessors. Solche Wartezeiten können entstehen, wenn während der Programmausführung Benutzereingaben nötig sind, bevor die Ausführung fortgesetzt werden kann oder wenn neue Daten aus dem vergleichsweise langsamen Hauptspeicher nachgeladen werden müssen, falls der prozessoreigene Cache nicht groß genug ist, um alle nötigen Daten für die aktuelle Ausführung auf einmal zu laden\cite[1135]{wolf2020}. Ohne Parallelität würden moderne Softwareanwendungen jeglicher Art nahezu unnutzbar werden. Einfache Vorgänge wie das Laden von Benutzerdaten aus einer lokalen Datenbank oder das Downloaden von Bildern aus dem Netz, würden ohne Parallelität beispielsweise zum Einfrieren der Benutzeroberfläche führen, da bei sequentiellen Programmabläufen alle Aufgaben strikt hintereinander ausgeführt werden müssen. Android selbst wäre ohne Parallelität nicht umsetzbar, da Androids Architektur Multithreading und damit Parallelität voraussetzt.

Für die Realisierung von Parallelität haben sich mit der Evolution der Prozessortechnologie verschieden Ansätze und Techniken entwickelt. Jede dieser Techniken ist bis heute relevant und glänzt in unterschiedlichen Anwendungsfällen.

\underline{Pipelining}

Beim Pipelining wird die Ausführung von Befehlen in verschiedene Phasen aufgeteilt, die jeweils durch eine eigene Ausführungseinheit bearbeitet werden. Sobald ein Befehl die aktuelle Phase abgeschlossen hat und zur nächsten Phase springt, kann bereits mit der Bearbeitung des nächsten Befehls in der frei gewordenen Phase begonnen werden. In \autoref{fig:Pipeline} ist ein Beispiel einer 5-Phasen Pipeline veranschaulicht. Die Bearbeitung jeder Phase dauert im Optimalfall einen Taktzyklus, da andernfalls die Pipeline bei der Bearbeitung der nachfolgenden Befehle in dieser Phase geblockt wird. Dies ist zum Beispiel in \autoref{fig:Pipeline} bei Befehl drei während der Execute Phase der Fall. Ein großer Nachteil von Pipelining tritt bei häufigen Programmsprüngen auf, da bei jedem Sprung die komplette Pipeline geleert werden muss und alle Phasen, die bis dorthin vollendet wurden, verworfen werden und umsonst bearbeitet wurden. Dieser Umstand ist besonders bei größeren Pipelines kritisch, da die einzelnen Befehle für längere Zeiträume in der Pipeline gehalten werden \cite{pipelineElektro}.
\begin{figure}[H]
	\begin{center}	 
	\includegraphics[scale=1]{Pipeline}
	\caption{Beispiel für eine 5-Phasen-Pipeline (Quelle: \cite{pipelineElektro})}
	\label{fig:Pipeline} 
	\end{center}
\end{figure}

\underline{simultanes Multithreading}

Ein Thread ist ein sequenzieller Ausführungspfaden innerhalb eines Prozesses, der ausgeführt wird. Bei Rechnern mit nur einem physischen Rechenkern kann zu einem Zeitpunkt immer nur ein Thread eines Prozesses gleichzeitig ausgeführt werden. Falls diese Ausführung durch Ereignisse wie Speicherzugriffe oder Nutzereingaben unterbrochen wird, blockiert der zugewiesene Thread die \ac{cpu} vollständig, sofern kein Multithreading betrieben wird. Durch sogenanntes simultanes Multithreading wird die verfügbare \ac{cpu} Rechenzeit auf mehrere Prozesse und Threads aufgeteilt. Die \ac{cpu} kann dadurch zwischen den Threads hin und her springen und so längere Wartezeiten verhindern. Dies steigert die Effizienz der \ac{cpu} hinsichtlich der Laufzeit sowie des Energieverbrauchs \cite[877]{cplusplus}. Diese Technik ermöglicht auch auf Systemen mit nur einem physischen Rechenkern den Eindruck von Parallelität und führt zu besser Nutzbarkeit der Anwendungen.

\underline{Hyperthreading}

Die Technologie Hyperthreading wurde von Intel mit den Prozessoren Pentium 3, Pentium 4 und Xeon eingeführt. Hierbei wird der Durchsatz von multithreaded Anwendungen im Multitasking erhöht, indem die Auslastung der On-Chip-Ressourcen erhöht wird, die in der Intel-NetBurst-Mikroarchitektur verfügbar sind. Ein typischer Thread belastet nur etwa 35 \% der NetBurst-Ausführungsressourcen. Hyperthreading erhöht die Auslastung durch notwendige Logik und Ressourcen, die der CPU hinzugefügt werden. Für die Aufteilung der reinkommenden Daten auf den freien Raum sorgen somit zwei logische Prozessoren, die vom Betriebssystem mittels klassischer Multiprocessing-Verfahren verwaltet werden \cite[1138]{wolf2020}. Somit stehen dem Rechner, trotz physischer Single-Core \ac{cpu}, daher zwei logische Rechenkerne zur Verfügung. Dies bietet die Möglichkeit Speicherwartezeit zu vermeiden, die ohne diese Technik die gesamte \ac{cpu} blockieren würde. Wenn der erste Thread im Wartezustand ist, kann der Prozessor mithilfe des zweiten logischen Kerns in der Programmausführung  der anderen Prozesse fortfahren.

\underline{Multicore-Prozessor}

Bei Multicore Systemen handelt es sich um Prozessoren mit mehreren physischen Rechenkernen. Diese können voneinander unabhängig arbeiten und ermöglichen echte Parallelität. Alle Rechenkerne nutzen zusätzlich die schon genannten Techniken, um möglichst effiziente Ergebnisse zu erhalten. Der Vorteil liegt hierbei nicht nur in der Steigerung der Geschwindigkeit. Mehrkernprozessoren können wesentlich geringer getaktet werden als Single-Core Prozessoren und ermöglichen trotzdem erhöhte Laufzeitgeschwindigkeit bei geringerer Leistungsaufnahme und Wärmeentwicklung \cite{MulicoreElektro}. Es könnte die Annahme getroffen werden, dass mit steigender Rechenkernzahl auch die Rechenleistung äquivalent ansteigt. Sodass die neue Laufzeit im Multi-Core-Betrieb gleich der ursprünglichen Laufzeit mit einem Kern geteilt durch die Anzahl der Rechenkerne ist. In der Realität ist dies jedoch nicht umsetzbar, da verschiedene Faktoren diese Annahme begrenzen. Zunächst einmal wächst mit steigender Anzahl an Rechenkernen auch der Aufwand der Verwaltung durch das Betriebssystem. Außerdem ist es nicht einfach den Zugriff auf geteilte Speicherressourcen durch mehrere Rechenkerne zu synchronisieren. Die Laufzeit für parallele Prozesse setzt sich grob aus folgenden Anteilen zusammen.
\begin{aligneddescription}
\item[Rechenzeit] Zeit für die Durchführung von Berechnungen unter Verwendung von Daten im lokalen Speicher der einzelnen Prozessoren.
\item[Kommunikationszeit] Zeit für den Austausch von Daten zwischen Prozessoren.
\item[Wartezeit] Z.B. aufgrund ungleicher Verteilung von Last zwischen den Rechenkernen, Datenabhängigkeiten im Algorithmus oder Ein- und Ausgabe.
\item[Synchronisationszeit] Zeit für die Synchronisation beteiligter Prozesse und von Ressourcenzugriffen.
\item[Platzierungszeit] Zeit für die Allokation der Tasks auf die einzelnen Prozessoren, sowie eine mögliche dynamische Lastverteilung zur Programmlaufzeit.
\item[Startzeit] Zeit zum Starten der parallelen Threads auf allen Rechenkernen.
\end{aligneddescription}\cite[313]{parallelBook}

Viele Anwendungen können nur für kleine Bestandteile ihrer Ausführung von den Vorteilen eines Multi-core Systems profitieren, da die meisten Anwendungsfälle durch Nutzeraktionen bestimmt sind. Bei Programmabläufen mit strikt aufeinanderfolgenden Abhängigkeiten ist Parallelität ohnehin nicht möglich und resultiert in traditionell sequentielle Abläufe. Das Admahl'sche Gesetz beschreibt genau diese Grenze. So ist der Beschleunigungsfaktor, auch Speedup genannt, mit zusätzlichen Rechenkernen durch sequentielle Anteile eines Problems begrenzt \cite[314]{parallelBook}. Das Admahl'sche Gesetz, benannt nach Gene Admahl, dient zur Vorhersage der maximal zu erwartenden Beschleunigung eines Algorithmus durch parallele Ausführung und wird wie folgt beschrieben.

\begin{aligneddescription}
\item[Speedup] Beschleunigungsfaktor der Rechenzeit durch $p$ Kerne $S_{p}(n)$
\item [sequentielle Laufzeit] Laufzeit bei Ausführung mit einem Rechenkern $T_{seq}(n)$
\item[Anzahl der Rechenkerne] Anzahl der an der parallelen Ausführung beteiligten Prozessorkernen $p$
\item[sequentieller Anteil] Anteil des Problems, welcher ausschließlich sequentiell ausführbar ist $f$. Es gilt $0\leq f \leq 1$ wobei $f = 1$ bedeuteten würde, dass das gesamte Problem, also 100 \% des Problems, sequentiell ausgeführt werden muss.
\item[paralleler Anteil] Anteil des Problems, welcher parallelisierbar ist $(1-f)$
\item[parallele Laufzeit] Laufzeit bei paralleler Ausführung mit $p$ Rechenkernen für ein Problem der Größe $n$ $T_{p}(n)$
\item[Problemgröße] Die Größe der Berechnung des Algorithmus $n$
\end{aligneddescription}
\begin{equation}\label{eq:Amdahlsche Gesetz}
S_{p}(n)=\frac{T_{seq}(n)}{T_{p}(n)} =
\frac{T_{seq}(n)}{f*T_{seq}(n) + \frac{ (1-f)*T_{seq} }{p}}
\end{equation}
\cite[317]{parallelBook}

Der Speedup $S_{ p }(n)$ aus \autoref{eq:Amdahlsche Gesetz} wird im Rahmen dieser Arbeit für die Ermittlung der Effizienz es Multithreadings benötigt, welche wie folgt beschrieben ist. 
\begin{equation}\label{eq:Effizienz}
E_{ p }(n) =\frac{ S_{ p }(n) }{p}
\end{equation}
\cite[316]{parallelBook}

Die Effizienz (\autoref{eq:Effizienz}) eines parallelen Programms gibt die relative Verbesserung des Speedups $S_{ p }(n)$ bezüglich der Anzahl $p$ der Prozessorkerne an, die bei der Ausführung genutzt wurden. Diese Größe wird im Verlauf der Untersuchung der folgenden Kapitel für den Vergleich von Laufzeiteffizienz und Energieeffizienz in Abhängigkeit der Thread Anzahl verwendet.

\section{Thread Pool Implementierung in Android}

Die \glqq EnergyEfficience\grqq{} App wurde vollständig in Java entwickelt. An dieser Stelle sei erwähnt, dass mit der immer stärkeren Kotlin-Ausrichtung des Android Frameworks neue Technologien hinsichtlich Multithreading und Synchronisation an Beliebtheit gewinnen. Kotlin bietet neben den traditionellen Java Techniken wie Threading, Callbacks und Futures auch sogenannte Kotilin Corountines. Durch Corountines können Ausführungen von längeren Funktionen beliebig pausiert werden, um anschließend mit anderen Aufgaben fortzufahren. Sobald die \ac{cpu} wieder freie Ressourcen hat, springt der Programmcounter an die Stelle der pausierten Funktion zurück und nimmt die Ausführung wieder auf. Dies verhindert blockierende Berechnungen, welche beispielsweise zur Einfrierung der \ac{ui} führen können. Hierbei reicht es das Schlüsselwort \glqq suspend\grqq{} bei der Deklaration der Funktion anzugeben. Verglichen mit der Implementierung von Callbacks ist diese Herangehensweise sehr einfach und schnell umzusetzen \cite{kotlin-corountines}. In Java wird Nebenläufigkeit traditionell mit der java.lang.Thread Klasse realisiert. Im  Konstruktor des Thread-Objekts wird eine Referenz auf ein Objekt vom Typ Runnable verlangt, welches den parallel auszuführenden Programmcode enthält. Das Runnable-Objekt implementiert diesen Code in der vom Rnnable-Interface definierten Methode \emph{run()}. Bei der Nutzung ist es wichtig, den Code nicht einfach durch Aufrufen dieser \emph{run()}-Methode zu starten. Dies würde zu einer normalen sequentiellen Ausführung führen. Um eine parallele Ausführung zu erreichen muss die \emph{start()}-Methode des entsprechenden Thread-Objekts aufgerufen werden. Dadurch wird für diesen Thread eine separate Ablaufumgebung mit den nötigen Systemressourcen erstellt. Nun wird automatisch die interne \emph{run()}-Methode des Thread-Objekts mit der Implementierung des Runnable-Objekts innerhalb der separaten Ablaufumgebung ausgeführt. Sobald die \emph{run()}-Methode terminiert, wird der Thread automatisch beendet und seine Systemressourcen werden freigegeben. Es ist außerdem möglich, eine eigene Klasse zu erstellen, die vom Typ \emph{Thread} erbt und den auszuführenden Code direkt in der eigenen \emph{run()}-Methode implementiert. Diese Variante ist jedoch weniger flexibel als das Benutzen von separaten Runnable-Objekten, da für jedes Problem eine erweiterte Thread-Klasse mit einer eigenen Variante der \emph{run()}-Methode erstellt werden müsste.  Bei der Ausführung von mehreren Threads zur gleichen Zeit, sind die genauen Terminierungszeitpunkte der einzelnen Threads selbst bei identischen Aufgaben nicht vorhersehbar, da der Kontextwechsel zwischen parallellaufenden Threads vom Scheduler des Betriebssystems organisiert wird und daher nicht nachvollziehbar ist \cite{javaistauchnurInsel}. Generell muss erwähnt werden, dass die \ac{jvm} die Thread-Verwaltung direkt auf das Betriebssystem abbildet. Das bedeutet, dass die eigentliche Thread-Verwaltung auf technischer Ebene inklusive der einhergehenden Ressourcenverwaltung nicht direkt durch die Java Implementierung beeinflusst werden kann, sondern allein durch das zugrunde liegende Betriebssystem bestimmt wird \cite{javaistauchnurInsel}. Sprachen wie C++ oder C sind für solche Aufgaben besser geeignet und ermöglichen einen maschinennäheren Zugriff und dadurch in der Regel performantere Lösungen.

Das Prinzip der Erstellung von Threads und der Kapselung des Ausführungscodes durch Runnables ist zwar die Grundlage für Multithreading in Java, reicht für die effektive Umsetzung von Threadpools für das dynamische Verwalten von vielen Threads alleine nicht aus. Die feste Bindung zwischen dem Ausführungs-Thread und dem Runnable-Objekt ist zu unflexible um ein effizientes und benutzerfreundliches Thread-Management zu realisieren. Schon bei der Erzeugung eines Threads muss das Runnable-Objekt im Thread-Konstruktor übergeben werden und kann im Nachhinein nicht mehr geändert werden. Des Weiteren ist es nicht möglich die \emph{start()}-Methode eines Threads mehrmals hintereinander aufzurufen, da dies zu einer Exeption führen würde falls der Thread bereits läuft. Weil das Thread-Objekt nach Abarbeitung des Runnables beendet und verworfen wird, ist es außerdem nicht möglich, Threads wiederzuverwenden. Das bedeutet, dass für jede Abarbeitung eines Raunnables ein neues Thread-Objekt erstellt werden muss. Selbst wenn die abzuarbeitende Aufgabe identisch ist, muss ein neues Objekt mit dem gleichen Runnable instanziiert werden. Das ständige erstellen und verwerfen von Thread-Objekten führt zu einer permanenten Belastung des Garbage Collectors und kann zu Performance-Verlust führen. Um diese ungünstigen Nebenerscheinungen der starren Kopplung von Thread und Runnable-Objekt zu umgehen, bietet Java die Schnittstelle \emph{Executor}, welche die Ausführung des Runnable-Programmcodes von der Initialisierung der Threads trennt. Dieses Interface schreibt die Methode \emph{execute(Runnable command)} vor,  mit der beliebig austauschbare Runnables auf einem \emph{Executer} ausgeführt werden können \cite{javaInselExecutor}. Für die Umsetzung des parallelen Base64-Encoders wurde ein Threadpool Manager mithilfe der \emph{ThreadPoolExecutor} Klasse umgesetzt. \emph{ThreadPoolExecutor} ist eine Java eigene Implementierung der Schnittstelle \emph{Executor}, um eine Sammlung von Threads aufzubauen, welche beliebig viele Aufgaben (Runnables) koordiniert abarbeiten kann. Dabei werden den Threads ohne Aufgabe neue Runnables aus einer Aufgaben-Queue dynamisch zugeordnet \cite{javaInselExecutor}. Der \autoref{lst:CustomThreadManager} zeigt die Implementierung des \emph{ CustomThreadPoolManagers} in der \glqq EnergyEfficience\grqq{} App. Im Konstruktor wird eine Instanz des \emph{ThreadPoolExecutors} initialisiert. Mit \emph{NUMBER\_OF\_CORES} wird die maximale Anzahl der gleichzeitigen Worker-Threads angegeben. Standardmäßig wird dieser Wert durch Aufruf der Methoden \emph{Runtime.getRuntime().availableProcessors()} mit der Anzahl der physischen Rechenkerne des Gerätes gleichgesetzt. Da die Untersuchung jedoch unter Anderem darauf abzielt, das Verhalten des Energieverbrauchs bei variabler Anzahl von Worker-Threads darzustellen, gibt es eine  \emph{ setNumberOfCores(int numberOfCores)}-Methode, um  diesen Wert während der Laufzeit anzupassen. Die \emph{ KEEP\_ALIVE\_TIME} Variable legt fest, wie lange ein Thread ohne Aufgaben am Leben erhalten wird, um auf neue Runnables zur Ausführung zu warten. Bekommt der Thread innerhalb dieser Zeit keine neue Aufgabe zugewiesen, so werden seine Ressourcen für andere Prozesse freigegeben. Über eine \emph{ BlockingQueue} (\emph{mTaskQueue}) werden die vom Thread-Pool abzuarbeitenden Runnables zwischengespeichert. Dies ist eine spezielle Datenstruktur, die Operationen unterstützt, welche nicht sofort ausgeführt werden können. So könnte es zum Beispiel vorkommen, dass ein Thread aus dem Thread-Pool mit der letzten Ausführung fertig ist und nun nach einer weiteren Aufgabe fordert, ohne dass neue Runnables in der \emph{ mTaskQueue} vorhanden sind. Die \emph{ BlockingQueue} bietet für solche Fälle die \emph{poll(time, unit)}-Methode an. Dadurch wird der aufrufende Thread für den angegebenen Zeitraum geblockt, in welchem er auf neue Aufgaben in Form von Runnables wartet \cite{BlockingQueue}. Dieses Zeitintervall wird in \autoref{lst:CustomThreadManager} durch die Variablen \emph{ KEEP\_ALIVE\_TIME} und \emph{ KEEP\_ALIVE\_TIME\_UNIT} festgelegt. Da es mit normalen Runnables nur schwer möglich ist, Ergebnisse einer asynchronen Methodenausführung in Form von normalen Rückgabewerten abzugreifen, wurden die Callables eingeführt. Callable-Objekte funktionieren ähnlich wie Runnable-Objekte und können daher auch wie Runnables behandelt werden. Allerdings implementieren sie statt der \emph{run()}-Methode die \emph{call()}-Methode. Wenn ein Callable-Objekt durch \emph{submit(Callable c)} der Aufgabenqueue (\emph{ mTaskQueue}) hinzugefügt wird, dann  liefert  dieser Aufruf ein Objekt vom Typ \emph{Future}. Dieses \emph{Future-Objekt} dient als Platzhalter für das zukünftige Ergebnis des asynchronen Aufrufs.  In der Methode \emph{ addCallable(Callable callable)} werden neue Aufgaben in Form von Callables der \emph{ mTaskQueue} hinzugefügt und gleichzeitig Future-Objekte für jedes dieser Callalbes in die \emph{ mTaskQueue} geschrieben. Mit dieser Struktur ist ein eleganter Zugriff auf die Ergebnisse der asynchronen Thread-Ausführungen möglich, ohne dabei die eigentliche Routine der Threads zu stören. Um zu verhindern, dass mehrere Instanzen dieses Threadpools gleichzeitig existieren können, wurde diese Klasse als sogenanntes Singleton implementiert. Singleton ist die Bezeichnung für ein Design Pattern aus der Softwareentwicklung, bei dem sichergestellt wird, dass von einer Klasse nur eine einzige Instanz existiert. Diese Instanz wird global definiert, sodass es an jeder Stelle im Projekt verfügbar ist.  Zur Umsetzung wurde der Konstruktor als \emph{private} definiert, um zu verhindern, dass weitere Instanzen außerhalb des Klassenkontextes erstellt werden können. In Zeile 14 von \autoref{lst:CustomThreadManager} wird einmalig eine statische Instanz von \emph{CustomThreadPoolManager} definiert, welche durch die statische Klassenmethode \emph{getInstance()} abrufbar ist. Die \emph{BackgroundThreadFactory} wird benötigt um die Erstellung und Zuweisung von neuen Threads mit Runnable-  beziehungsweise Callable-Objekten zu automatisieren.

\begin{lstlisting}[language=java,caption={der CustomThreadManager aus der EnergyEfficience App},label=lst:CustomThreadManager]
public class CustomThreadPoolManager {

    private  static int NUMBER_OF_CORES = Runtime.getRuntime().availableProcessors();
    private static final int KEEP_ALIVE_TIME = 1;
    private  static final TimeUnit KEEP_ALIVE_TIME_UNIT;
    private Handler mainThreadHandler = HandlerCompat.createAsync(Looper.getMainLooper());
    private final ExecutorService mExecuterService;
    private final BlockingQueue<Runnable> mTaskQueue;
    private List<Future> mRunningTaskList;
    private static CustomThreadPoolManager singleInstance = null;

    static{
        KEEP_ALIVE_TIME_UNIT = TimeUnit.SECONDS;
        singleInstance = new CustomThreadPoolManager();
    }
    private CustomThreadPoolManager(){
        mTaskQueue = new LinkedBlockingQueue<Runnable>();
        mRunningTaskList = new ArrayList<>();
        mExecuterService = new ThreadPoolExecutor(
                NUMBER_OF_CORES,
                NUMBER_OF_CORES,
                KEEP_ALIVE_TIME,
                KEEP_ALIVE_TIME_UNIT,
                mTaskQueue,
                new BackgroundThreadFactory());
    }
    public static void setNumberOfCores(int numberOfCores) {
        if(numberOfCores > 0){
            NUMBER_OF_CORES = numberOfCores;
            singleInstance = new CustomThreadPoolManager();
        }
    }
    public void addCallable(Callable callable){
        Future future = mExecuterService.submit(callable);
        mRunningTaskList.add(future);
    }
    public Handler getMainThreadHandler(){
        return this.mainThreadHandler;
    }
    public static CustomThreadPoolManager getInstance(){
        return singleInstance;
    }
    public int getNumberOfCores(){
        return NUMBER_OF_CORES;
    }
    public void cancelAllTasks() {
        synchronized (this) {
            mTaskQueue.clear();
            for (Future task : mRunningTaskList) {
                if (!task.isDone()) {
                    task.cancel(true);
                }
            }
            mRunningTaskList.clear();
        }
    }
    private static class BackgroundThreadFactory implements ThreadFactory {
        private static int sTag = 1;
        @Override
        public Thread newThread(Runnable runnable) {
            Thread thread = new Thread(runnable);
            thread.setName("CustomThread" + sTag);
            sTag++;
            thread.setPriority(THREAD_PRIORITY_BACKGROUND);
            thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
                @Override
                public void uncaughtException(Thread thread, Throwable ex) {
                    Log.e("ThreadFactory", thread.getName() + " encountered an error: " + ex.getMessage());
                }
            });
            return thread;
        }
    }
}
\end{lstlisting}

Die Flexibilität dieses Threadpools, der in der variablen Festlegung der Arbeiter Threads liegt, bildet die Grundlage der folgenden Messungen. Ziel ist es, herauszufinden, welche Anzahl von Threads am energieeffizientesten ist und welcher Zusammenhang mit der Laufzeit besteht.

\section{Ein paralleler Base64 Encoder}

Für die Untersuchung der parallelen Ausführung über beliebig viele Threads wurde die Base64-Kodierung gewählt. Hierbei werden 8-Bit-Binärdateien in eine 7-Bit-\ac{ascii}-Textrepräsentation umgewandelt. So können binäre Dateiformate wie beispielsweise Bilddateien in \ac{ascii}-Textformate umgewandelt werden und direkt in \ac{html}- oder \ac{css}-Dateien eingebunden werden. Auch für das Verschicken von E-Mails mit Anhang wird die Base64-Kodierung noch häufig genutzt, da das \ac{smtp} ursprünglich nur in der Lage war, sieben-Bit-\ac{ascii}-Texte zu transportieren. Ohne die Base64-Kodierung wäre es daher Schwierig, empfangene Texte aus anderen Regionen mit verschiedenen Zeichensätzen vernünftig darzustellen. Um Komplikationen durch die Übertragung von länderspezifischen Sonderzeichen oder Steuerzeichen zu verhindern, verwendet Base64 ausschließlich Buchstaben des nicht erweiterten Alphabets (A bis Z, a bis z), die Ziffern von Eins bis Null sowie die Zeichen \enquote{/} und \enquote{+} für die Kodierung. Diese Zeichen sind standardisiert und kommen in den meisten länderspezifischen beziehungsweise betriebssystemspezifischen Zeichensätzen vor. Bei der Kodierung von \ac{ascii}-Dateien wird wie folgt vorgegangen. Ein \ac{ascii}-Zeichen wird standardmäßig durch acht Bits repräsentiert. Eine \ac{ascii}-Datei wird zunächst in Binärcode umgewandelt, welcher eine Aneinanderreihung der 8-Bit-Pakete der einzelnen \ac{ascii}-Zeichen ist. Nun wird diese Bytefolge in Abschnitte von jeweils sechs Bits aufgeteilt. Diese 6-Bit-Abschnitte werden entsprechend ihres umgerechneten Zahlenwertes mithilfe einer einfachen Zeichentabelle in Base64-Code umgewandelt. Mit sechs Bits lassen sich $2^{ 6 } = 64$ verschieden Zustände darstellen, daher besteht die Base64-Umwandlungstabelle aus 64 verschiedenen Zeichen. Diese Tabelle und die Spezifikation des Base-64-Standards sind im \ac{rfc}-4648 \cite{base64-rfc4648} und \ac{rfc}-2045 festgelegt \cite{base64-rfc2045}. Die Komplexität der Base64-Kodierung hängt von der Größe der zu kodierenden Daten ab, also von der Anzahl der Bits der entsprechenden Binärcoderepräsentation. Dabei verhält sich die Laufzeitkomplexität der Kodierung proportional zur Anzahl der Bits. Die Umwandlungsoperation ist mithilfe einer iterativen Schleife umsetzbar. Daher ist die Base64-Kodierung in die Komplexitätsklasse $O(n)$ der O-Notation einzuordnen. Die O-Notation zur Beschreibung der Komplexität von Algorithmen wurde von Donald E. Knuth in seinem Werk \glqq The art of computer programming: Fundamental algorithms\grqq{} vorgestellt \cite[107]{knuth1}.

Im folgenden Codeausschnitt aus der \glqq EnergyEfficience\grqq{} Applikation, ist zu sehen, dass die Base64-Kodierung innerhalb einer \emph{Callable}-Klasse implementiert ist. Dadurch können mithilfe des Threadpools beliebig viele Kodierungen parallel durchgeführt werden, indem neue \emph{Base64EncodeCallable} Objekte in die Aufgaben-Queue des Threadpools hinzugefügt werden. Über den Konstruktor kann die Größe des zu kodierenden Textausdrucks festgelegt werden, welcher durch einen \emph{StringBuilder} in Zeile 27 generiert wird. Damit es nicht zu Abstürzen aufgrund von \ac{oom} Fehlern kommt, wurde die maximale Größe des Strings auf 10000 \ac{kb} beschränkt. In Zeile 12 wird dieser String in seine Byte-Repräsentation umgewandelt, damit dieser durch den Base64-Encoder kodiert werden kann. Um die Ergebnisse der Kodierung an anderen Stellen der Applikation nutzbar zu machen, muss eine Kommunikation zwischen dem Thread, der ein \emph{Base64Callable} bearbeitet, und dem \ac{ui}-Thread hergestellt werden.  Androids \ac{ui}-Thread besitzt eine sogenannte \emph{Message Queue}, welche alle Aktionen, die auf dem \ac{ui}-Thread ausgeführt werden sollen enthält. Dies können Interaktionen mit der \ac{ui} oder einfache Funktionsaufrufe sein. Diese \emph{Message Queue} wird ständig von einem assoziierten \emph{Looper}-Objekt durchlaufen. Es handelt sich hierbei im Grunde genommen um eine Endlosschleife, welche alle Objekte der \emph{Message Queue} durchläuft und die Aktion auswählt, die auf dem \ac{ui}-Thread als Nächstes bearbeitet werden soll. Die Reihenfolge der Durchführung dieser Aktionen wird nach der Priorität und einer Zeitmarke der einzelnen Aktionen festgelegt. Nun ist es möglich durch einen \emph{Handler}, der mit dieser \emph{Message Queue} gekoppelt ist, Einfluss auf die Queue des \ac{ui}-Threads zu nehmen und neue Aufgabenpakete in diese Queue einzuschieben \cite{Handler}. Dieses Prinzip wird in Zeile 27 der \emph{notifyResults}-Methode genutzt. Mit \emph{resultHanlder.post()} wird ein neues Runnable in die \emph{Message Queue} des \ac{ui}-Threads eingefügt. Über ein Callback wird so die Methode \emph{onComplete(result)} auf dem \ac{ui}-Thread ausgeführt. Somit kann das Ergebnis der Kodierung in den Kontext des \ac{ui}-Threads zurückgegeben werden. Außerdem findet über diese Callback-Methode auch die Synchronisation der parallelen Threads sowie die Zeitmessung der Ausführung statt. 

\begin{lstlisting}[language=java,caption={Base64-Callable aus derEnergyEfficience App},label=lst:Base64Callable]
public class Base64EncodeCallable implements Callable {
    private int msgSize = 0; //in KB
    private Base64Callback callback;
    private Handler resultHandler;
    public Base64EncodeCallable(int msgSize, Base64Callback callback, Handler resultHandler){
        this.callback = callback;
        this.resultHandler = resultHandler;
        this.msgSize = msgSize;
    }
    @Override
    public Object call() throws Exception {
        byte[] buffer = createBigString(msgSize).getBytes();
        notifyResult(Base64.getEncoder().encodeToString(buffer), callback, resultHandler);
        return null;
    }
    private void notifyResult(final String result, final Base64Callback callback, final Handler resultHandler){
        resultHandler.post(new Runnable() {
            @Override
            public void run() {
                callback.onComplete(result);
            }
        });
    }
    public String createBigString(int msgSize){
        msgSize = msgSize / 2;
        msgSize = msgSize* 1024;
        StringBuilder sb = new StringBuilder(msgSize);
        for(int i = 0; i< msgSize; i++){
            sb.append('a');
        }
        return sb.toString();
    }
}
\end{lstlisting}

\section{Iteration und Rekursion Vorbetrachtung}

Als Iteration wird ein mehrmaliges Ausführen einer Aktion oder Berechnung bezeichnet, welche durch eine Abbruchbedingung oder maximale Anzahl an Ausführungen begrenzt ist. In der Programmierung werden Iterationen hauptsächlich mit Kontrollstrukturen wie Schliefen realisiert \cite{IterationUndRekursion}.

Auch die Rekursion beschreibt eine wiederholte Ausführung, doch werden hierbei keine Zählschleifen verwendet. Rekursionen werden durch Funktionen definiert, die sich selbst solange aufrufen, bis eine Abbruchbedingung erfüllt ist. Es ist allgemein bekannt, dass die rekursive Beschreibung eines Algorithmus zwar übersichtlicher und wesentlich kürzer ausfällt, dafür aber mehr Arbeitsspeicher verbraucht \cite{IterationUndRekursion}. Dies liegt an der Tatsache, dass Funktionsaufrufe einen Overhead an Daten  erzeugen, welcher für das Betriebssystem notwendig zur Koordination des Programmablaufs ist. Für die Realisierung von Funktionsaufrufen in einem Programm müssen in der Regel folgende Schritte bewältigt werden.

\begin{itemize}
\item Die Funktionsparameter müssen entsprechend ihrer Beschreibung eine Speicheradresse  erhalten, welche ausreichend groß für deren Typen ist

\item Der Rückgabewert der Funktion benötigt ebenfalls eine passende Speicherreservierung.
\item Der Funktionsname muss aufgelöst werden, um die richtige Stelle im Speicher zu finden, auf welche der \emph{Program Counter} zur Ausführung der Funktion springen muss.
\item Die Parameter und die \emph{return}-Adresse des Funktionsaufrufs \footnote{Speicheradresse an die nach der Ausführung der Funktion zurückgesprungen werden soll und das Ergebnis gespeichert wird} müssen auf den \emph{Stack} geschrieben werden.
\item Nachdem alle nötigen Daten auf dem \emph{stack} platziert sind, springt der \emph{Program Counter} zur Adresse der Funktion um die Ausführung zu ermöglichen.
\item Nach der erfolgreichen Abarbeitung der Funktion, müssen die Parameter und der Rückgabewert der Funktion wieder vom \emph{Stack} entfernt werden und das Ergebnis wird an der \emph{return}-Adresse gespeichert. 
\end{itemize}

Die meisten dieser Operationen sind optimiert und kosten sehr wenig Zeit. Trotzdem kann es bei hoher Rekursionstiefe zu erheblicher Speicherallokation kommen, da mit jedem Rekursionsschritt ein neuer Funktionsaufruf nötig ist und somit Overhead produziert wird, welcher erst nach Beendigung der Rekursion wieder freigegeben wird. Iterative Implementierungen sind zwar aufwendiger zu entwickeln, benötigen diesen Overhead jedoch nicht. Ob sich dieser Effekt auch signifikant auf den Energieverbrauch auswirkt, wird durch die Betrachtung in Kapitel 5 untersucht.

\section{Gegenüberstellung anhand verschiedener Mergesort Varianten}
\subsection{Rekursiver Mergesort}

Um die rekursive mit der iterativen Ausführung  sinnvoll vergleichen zu können, wird ein Algorithmus benötigt, welcher sowohl mit seiner iterativen als auch mit seiner rekursiven Version in der selben Komplexitätsklasse  der $O-$Notation nach  Donald E. Knuth liegt. Für diese Untersuchung wurde für diesen Zweck der Mergesort Algorithmus ausgewählt. Es handelt sich hierbei um ein stabiles Sortierverfahren \footnote{Stabile Sortierverfahren behalten die relative Reihenfolge von gleichgroßen Elementen bei.}, welches im Worst-, Best- und Average-Case die Laufzeitkomplexität von $O(n*log(n))$ besitzt \cite[96]{AlgorithmenDatenstrukturen}. Außerdem gibt es rekursive und iterative Versionen des Algorithmus mit der selben Laufzeitkomplexität \cite[134 f.]{AlgorithmenJurgen}. Der Nachteil des hier verwendeten Verfahrens ist, dass es vergleichsweise viel Speicher benötigt, da es sich um ein \emph{out-of-place} Verfahren handelt. Das bedeutet, dass für die Durchführung der Sortierung eine gesonderte Speicherung der Daten zur Bearbeitung notwendig ist und nicht ausschließlich mit dem Speicherbedarf der Eingabedaten gearbeitet wird.

Der Mergesort ist eine Anwendung des \emph{Teile und herrsche Verfahrens} aus der Informatik. Hierbei geht es darum, ein komplexes Hauptproblem solange in seine Teileprobleme aufzuteilen, bis die einzelnen Teilprobleme mit geringem Aufwand lösbar sind. Anschließend werden die gelösten Teilprobleme wieder zusammengeführt, bis die Gesamtlösung des Hauptproblems erreicht wurde \cite[9]{AlgorithmenDatenstrukturen}. Der Mergesort setzt dieses Prinzip mithilfe von drei Teilschritten um.
\begin{enumerate}
\item Die zu sortierende Menge wird solange zerlegt, bis nur noch einelementige Mengen vorhanden sind.
\item Die Teilmengen werden nun einzeln während der Zusammenführung sortiert
\item Die so entstandenen sortierten Teilmengen müssen zum Schluss mithilfe eines Suchkriteriums wieder zusammengeführt werden 
\end{enumerate}

In \autoref{lst:MergeSortRekursiv} ist die rekursive Java-Implementierung zu sehen, welche in der \glqq EnergyEfficience\grqq{} Applikation verwendet wurde. Zunächst wird ein Array vom Typ \emph{int} im Konstruktor übergeben, welches die Eingabemenge enthält. In Zeile sieben ist zu erkennen, dass der Mergesort ein Hilfs-Array der Größe der Eingabedaten benötigt. In der \emph{sort(int l, int r)}-Methode findet die Aufteilung der Eingabemenge in seine Teilmengen statt. Hierfür werden mit $l$ und $r$ jeweils der Anfangs- und Endindex des zu sortierenden Arrays angegeben. Mit diesen beiden Werten wird der Mittelindex $q$ der Eingabemenge ermittelt. Nun erfolgen die rekursiven Aufrufe der \emph{sort}-Methode um jeweils die linke und die rechte Hälfte der Eingabemenge zu sortieren. Zum Schluss wird für den aktuellen Abschnitt die \emph{merge}-Methode aufgerufen. Diese ist für die rekursive Zusammenführung der einzelnen Teilstücke zuständig. Im Zuge dieser Zusammenführung werden die einzelnen Teilstücke sortiert. Diese Rekursion wird solange fortgesetzt, bis die Bedingung $l < r$ nicht mehr erfüllt wird. Sobald dieser Punkt erreicht ist, wurde die Eingabemenge in seine einelementigen Teilmengen gespalten und der Funktionsstack kann abgearbeitet werden. In der \emph{merge}-Methode werden drei Schleifen benötigt. Die erste Schleife initialisiert den Hilfs-Array $arr[]$ mit der linken Hälfte der aktuellen Teilmenge aus dem Eingabe-Array $intArr[]$. Hierzu werden die Parameter $l$ und $q$ als Indexgrenzen verwendet. Analog wird in der zweiten Schleife die Rechte Hälfte der aktuellen Teilmenge in den Hilfs-Array geschrieben. Die vierte for-Schleife dient der Zusammenführung der beiden Hälften. Dabei wird aufsteigend der Größe nach sortiert und in den Haupt-Array $intArr[]$ zurückgeschrieben.

\begin{lstlisting}[language=java,caption={rekursiver Merge sort (Quelle: \cite{MergeSortRekursiv})},label=lst:MergeSortRekursiv]
public class MergeSortImplementation {
    public static int[] intArr;
    public static int[] arr;

    public MergeSortImplementation(int[] intArr) {
        this.intArr = intArr;
        this.arr = new int[intArr.length];
    }

    public int[] sort(int l, int r) {
        if (l < r) {
            int q = (l + r) / 2;
            sort(l, q);
            sort(q + 1, r);
            merge(l, q, r);
        }
        return intArr;
    }

    private void merge(int l, int q, int r) {
        int i, j;
        for (i = l; i <= q; i++) {
            arr[i] = intArr[i];
        }
        for (j = q + 1; j <= r; j++) {
            arr[r + q + 1 - j] = intArr[j];
        }
        i = l;
        j = r;
        for (int k = l; k <= r; k++) {
            if (arr[i] <= arr[j]) {
                intArr[k] = arr[i];
                i++;
            } else {
                intArr[k] = arr[j];
                j--;
            }
        }
    }
}
\end{lstlisting}

\subsection{Iterativer Mergesort}

In \autoref{lst:MergeSortIterativ} ist eine iterative Variante des Mergesorts zu sehen. Es ist zu erkennen, dass die iterative Version nicht ganz so intuitiv zu verstehen ist wie die Rekursion. Die Erfahrung zeigt, dass iterative Lösungen meist aufwendiger zu entwickeln sind als rekursive. Dafür verlangen iterative Implementierung weitaus weniger Speicher während der Ausführung. Auch in \autoref{lst:MergeSortIterativ} werden neben dem Konstruktor zwei Methoden für den eigentlichen Algorithmus benötigt. Die $mergesort$-Methode implementiert zwei ineinander verschachtelte Schleifen. Die äußere Schleife iteriert über alle Elemente des Eingabearrays. In der Inneren Schleife wird für einen stetig wachsenden Bereich der Eingabemenge, die $merge$-Methode aufgerufen. Dabei wird der Hauptarray $a[]$ von rechts nach links, Stück für Stück sortiert. Nach jeder Iteration wird der an  die $merge$-Methode übergebene Bereich erweitert. Die zusätzlichen Elemente werden dann in die schon sortierte Teilmenge an die richtige Stelle eingefügt. Die $merge$-Methode funktioniert prinzipiell genauso wie die der rekursiven Implementierung.

\begin{lstlisting}[language=java,caption={iterativer Mergesort (Quelle: \cite{MergeSortIterativ})},label=lst:MergeSortIterativ]
public class MergeSortImplementationIterative {
    private int[] a;
    private int[] b;    // Hilfsarray
    private int n;

    public void sort(int[] a) {
        this.a = a;
        n = a.length;
        b = new int[n / 2];
        mergesort();
    }

    private void mergesort() {
        int m, s;
        for (s = 1; s < n; s += s)
            for (m = n - 1 - s; m >= 0; m -= s + s)
                merge(max(m - s + 1, 0), m, m + s);
    }

    void merge(int lo, int m, int hi) {
        int i, j, k;
        i = 0;
        j = lo;
        // vordere Hälfte von a in Hilfsarray b kopieren
        while (j <= m)
            b[i++] = a[j++];
        i = 0;
        k = lo;
        // jeweils das nächstgrößte Element zurückkopieren
        while (k < j && j <= hi)
            if (b[i] <= a[j])
                a[k++] = b[i++];
            else
                a[k++] = a[j++];
        // Rest von b falls vorhanden zurückkopieren
        while (k < j)
            a[k++] = b[i++];
    }

    private int max(int a, int b) {
        return a > b ? a : b;
    }
}
\end{lstlisting}
\subsection{Paralleler Mergesort}

Für den parallelen Mergesort wurde wurde ein \emph{ForkJoinPool} implementiert. Hierbei handelt es sich um eine spezielle Form des \emph{ExecuterService}, welche für die Ausführung von parallelen, rekursiven Algorithmen optimal geeignet ist. Für den \emph{ForkJoinPool} gibt es spezielle Ableitungen der \emph{Runnable}-Klasse um das Prinzip des \glqq work-stealing's\grqq{} umzusetzen \cite{ForkJoinPoolOracle}. Diese Klassen sind zum einen \emph{ForkJoinTask<T>} und zum anderen \emph{RecursiveAction<T>}, welche in \autoref{lst:MergeSortParallel} verwendete wurde. In der \emph{compute}-Methode ist zu sehen, dass für jede Teilung der Eingabemenge ein neues Objekt vom Typ \emph{RecursiveAction} erzeugt wird. Diese Objekte fungieren als \emph{Runnable} für den \emph{ForkJoinPool}. Die Besonderheit dieser Struktur ist, dass jedes Objekt vom Typ \emph{RecursiveAction} seine eigene Queue hat, in welche durch Rekursion wiederum neue Aufgabenpakete vom Typ \emph{RecursiveAction} hinzugefügt werden können. Der \emph{ForkJoinPool} ist in der Lage auf jede dieser Queues zuzugreifen und bei Bedarf Aufgabenpakete aus diesen Queues an einen freien Thread zuzuteilen. Außerdem kümmert sich der \emph{ForkJoinPool} um die Synchronisierung der Zusammenführung der einzelnen Teilergebnisse der terminierten \emph{RecursiveActions} \cite{ForkJoinPoolExplained}. Damit von der parallelen Ausführung der einzelnen Teilaufgaben der Rekursion auch wirklich profitiert werden kann, muss eine Obergrenze für die Teilung der Eingabemenge gesetzt werden. in Zeile sechs ist zu sehen, dass diese Grenze auf 8192 Elemente angegeben wurde. Dies bedeutet, dass die Eingabemenge nur solange in Teilmengen zerlegt wird, bis die einzelnen Teilmengen aus maximal 8192 Elementen bestehen. Ist dieser Punkt erreicht, werden diese Teilmengen nach dem sequentiellen Verfahren aus \autoref{lst:MergeSortRekursiv} sortiert. Dieser Schritt ist notwendig, da es zu ressourcenaufwendig wäre, für jedes Element ein vergleichsweise speicherintensives \emph{RecursiveAction}-Objekt zu erstellen. Ansonsten würde die Ausführung sogar langsamer ausfallen als die rein sequentielle Sortierung.

\begin{lstlisting}[language=java,caption={paralleler Mergesort (Quelle: \cite{MergeSortParallel})},label=lst:MergeSortParallel]
public class ParallelMergeSort extends RecursiveAction {
    private final int[] array;
    private  final int[] helper;
    private  final int low;
    private final int high;
    private final int MAX = 8192;
    public ParallelMergeSort(final int[] array, final int[] helper, final int low, final int high){
        this.array = array;
        this.low = low;
        this.high = high;
        this.helper = helper;
    }
    @Override
    protected void compute() {
        if (low < high) {
            if (high - low <= MAX) { // Sequential implementation
                 sort(low, high);
            } else { // Parallel implementation
                final int middle = (low + high) / 2;
                final ParallelMergeSort left =
                        new ParallelMergeSort(array,helper, low, middle);
                final ParallelMergeSort right =
                        new ParallelMergeSort(array,helper,middle + 1, high);
                invokeAll(left, right);
                merge(low, middle, high);
            }
        }
    } 
    
/*****Nutzung der der RucursiveAction******/
    
    ForkJoinPool forkJoinPool = new ForkJoinPool(Runtime.getRuntime().availableProcessors() - 1);
    forkJoinPool.invoke(new ParallelMergeSort(...));
\end{lstlisting}